{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d6133508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('Mock_data.json') as json_data:\n",
    "    mock_data = json.load(json_data)\n",
    "    json_data.close()\n",
    "\n",
    "def write_to_json(dict=mock_data,filepath='Mock_data.json'):\n",
    "    with open(filepath,'w') as file:\n",
    "        json.dump(dict, file,indent=4)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73b452e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "e = 2.7182818\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (e**x) / (1 + (e**x))\n",
    "\n",
    "def tanh(x):\n",
    "    return ((e**x) - (e**-x))/ ((e**x)+(e**-x))\n",
    "\n",
    "def normalize(array,natural_numbers=True):\n",
    "    min = array[0]\n",
    "    max = array[0]\n",
    "    for n in array[1:]:\n",
    "        if n < min: min = n\n",
    "        if n > max: max = n\n",
    "\n",
    "    result = {\n",
    "        \"min\":min,\n",
    "        \"max\":max,\n",
    "        \"data\":[]\n",
    "    }\n",
    "    for n in array:\n",
    "        n = (n-min)/(max-min)\n",
    "        if not natural_numbers:\n",
    "            n = (2*n)-1\n",
    "        result['data'].append(n)\n",
    "\n",
    "    return result\n",
    "\n",
    "def denormalize(array,max,min,natural_numbers=True):\n",
    "    result = []\n",
    "    for n in array:\n",
    "        if not natural_numbers:\n",
    "            n = (n + 1)/2\n",
    "        n = (n*(max-min))+min\n",
    "        result.append(n)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.cell_state = 0\n",
    "        self.short = 0\n",
    "\n",
    "        # variables for forgetting long-term memory\n",
    "        self.FG_input_weight = 1\n",
    "        self.FG_short_weight = 1\n",
    "        self.FG_bias = 0\n",
    "\n",
    "        # variables for input gate\n",
    "        self.IG_input_weight = 1\n",
    "        self.IG_short_weight = 1\n",
    "        self.IG_bias = 0\n",
    "        # forget gate within input gate\n",
    "        self.IGF_input_weight = 1\n",
    "        self.IGF_short_weight = 1\n",
    "        self.IGF_bias = 0\n",
    "\n",
    "        # variables for forget gate within output gate\n",
    "        self.OGF_input_weight = 1\n",
    "        self.OGF_short_weight = 1\n",
    "        self.OGF_bias = 0\n",
    "\n",
    "\n",
    "    ## returns forget gate modifier\n",
    "    def forget_gate(self,input,input_weight=1,short_weight=1,bias=0):\n",
    "        weighted_input = input*input_weight\n",
    "        weighted_short = self.short*short_weight\n",
    "        sum = weighted_short + weighted_input + bias\n",
    "        result = sigmoid(sum)\n",
    "        return result\n",
    "\n",
    "    ## returns value to add to cell state\n",
    "    def input_gate(self,input):\n",
    "        weighted_input = input * self.IG_input_weight\n",
    "        weighted_short = self.short * self.IG_short_weight\n",
    "        sum = weighted_short + weighted_input + self.IG_bias\n",
    "        forget_modifier = self.forget_gate(input, self.IGF_input_weight, self.IGF_short_weight, self.IGF_bias)\n",
    "        result = tanh(sum) * forget_modifier\n",
    "        return result\n",
    "\n",
    "    ## returns new hidden state\n",
    "    def output_gate(self,input):\n",
    "        forget_modifier = self.forget_gate(input, self.OGF_input_weight, self.OGF_short_weight, self.OGF_bias)\n",
    "        output = tanh(self.cell_state) * forget_modifier\n",
    "        return output\n",
    "    \n",
    "    def forward(self,input):\n",
    "        self.cell_state *= self.forget_gate(input, self.FG_input_weight, self.FG_short_weight, self.FG_bias)\n",
    "        self.cell_state += self.input_gate(input)\n",
    "        self.short = self.output_gate(input)\n",
    "        print(\"Pass Complete:\",\n",
    "            \"\\n\\tInput -> \",input,\n",
    "            \"\\n\\tNew Short -> \",self.short,\n",
    "            \"\\n\\tNew Long -> \",self.cell_state)\n",
    "\n",
    "    # inputs should be normalized \n",
    "    def training_step(self,epoch):\n",
    "        for time_step in epoch:\n",
    "            self.forward(time_step)\n",
    "        print(\"*Sequence Unroll Complete*\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70923b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Unrolled: \n",
      "\tInput ->  -0.5555555555555556 \n",
      "\tNew Short ->  -0.06633219685645585 \n",
      "\tNew Long ->  -0.18399166560155847\n",
      "Input Unrolled: \n",
      "\tInput ->  -1.0 \n",
      "\tNew Short ->  -0.06247054020603262 \n",
      "\tNew Long ->  -0.24894755799539375\n",
      "Input Unrolled: \n",
      "\tInput ->  0.11111111111111116 \n",
      "\tNew Short ->  -0.052368031275773816 \n",
      "\tNew Long ->  -0.10260840283560163\n",
      "Input Unrolled: \n",
      "\tInput ->  0.7777777777777777 \n",
      "\tNew Short ->  0.22592243298435377 \n",
      "\tNew Long ->  0.3487847939286669\n",
      "Input Unrolled: \n",
      "\tInput ->  -0.7777777777777778 \n",
      "\tNew Short ->  -0.020427266010980043 \n",
      "\tNew Long ->  -0.05595696822618346\n",
      "Input Unrolled: \n",
      "\tInput ->  0.0 \n",
      "\tNew Short ->  -0.018698382658411384 \n",
      "\tNew Long ->  -0.037800644431316976\n",
      "Input Unrolled: \n",
      "\tInput ->  -0.33333333333333337 \n",
      "\tNew Short ->  -0.06358581553944426 \n",
      "\tNew Long ->  -0.1552369873116827\n",
      "Input Unrolled: \n",
      "\tInput ->  -0.6666666666666667 \n",
      "\tNew Short ->  -0.08058154610605835 \n",
      "\tNew Long ->  -0.2531069108281762\n",
      "Input Unrolled: \n",
      "\tInput ->  0.11111111111111116 \n",
      "\tNew Short ->  -0.05711554614784361 \n",
      "\tNew Long ->  -0.11299215163685734\n",
      "Input Unrolled: \n",
      "\tInput ->  0.33333333333333326 \n",
      "\tNew Short ->  0.0504383117939742 \n",
      "\tNew Long ->  0.08893697274944977\n",
      "Input Unrolled: \n",
      "\tInput ->  -0.2222222222222222 \n",
      "\tNew Short ->  -0.01695774941241449 \n",
      "\tNew Long ->  -0.0371107682442228\n",
      "Input Unrolled: \n",
      "\tInput ->  -0.5555555555555556 \n",
      "\tNew Short ->  -0.07115606590129576 \n",
      "\tNew Long ->  -0.19991707362818772\n",
      "Input Unrolled: \n",
      "\tInput ->  1.0 \n",
      "\tNew Short ->  0.2600184277614166 \n",
      "\tNew Long ->  0.38002403273673135\n",
      "Input Unrolled: \n",
      "\tInput ->  0.22222222222222232 \n",
      "\tNew Short ->  0.2915045983058402 \n",
      "\tNew Long ->  0.5119705011803486\n",
      "Input Unrolled: \n",
      "\tInput ->  -0.4444444444444444 \n",
      "\tNew Short ->  0.07613062473427855 \n",
      "\tNew Long ->  0.1663601172581248\n",
      "Input Unrolled: \n",
      "\tInput ->  0.33333333333333326 \n",
      "\tNew Short ->  0.1931217960083995 \n",
      "\tNew Long ->  0.33315836655339715\n",
      "*Sequence Unroll Complete*\n",
      "Prediction:  [0.1931217960083995] ----> [15.738096164075596]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    model = LSTM()\n",
    "    model.training_step(mock_data[\"normalized_-1to1\"][\"data\"])\n",
    "    print(\"Prediction: \",[model.short],\"---->\",denormalize([model.short],23,5,False))\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QuantumIntro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
